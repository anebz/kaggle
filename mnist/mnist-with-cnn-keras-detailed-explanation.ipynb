{"cells":[{"metadata":{"_uuid":"a4229d26429f6fdf9dd42b26b8b02a4c4175b707"},"cell_type":"markdown","source":"# MNIST with CNN (Keras) - Detailed explanation\n### **Ane Berasategi** - 25/07/2018\n\n* **0. Introduction**\n\n* **1. Data pre-processing**\n    * 1.1. Load data\n    * 1.2. Check shape, data type\n    * 1.3. Extract xtrain, ytrain\n    * 1.4. Mean and std of classes\n    * 1.5. Check nuls and missing values\n    * 1.6. Visualization\n    * 1.7. Normalization\n    * 1.8. Reshape\n    * 1.9. One hot encoding of label\n    * 1.10. Split training and validation sets  \n    \n* **2. CNN**\n    * 2.1. Define model\n    * 2.2. Set optimizer and learning rate annealer\n    * 2.3. Data augmentation\n    * 2.4. Fit model\n    \n* **3. Model evaluation and plots**\n    * 3.1. Plot training loss and evaluation loss\n    * 3.2. Plot confusion matrix\n    * 3.3. Plot errors\n\n* **4. Predict and save to csv**\n    * 4.1. Predict\n    * 4.2. Save to csv\n\n    "},{"metadata":{"_uuid":"d5780ab91c4dbd65ba303299ef03e596f55fff5c"},"cell_type":"markdown","source":"# 0. Introduction\n\nThis is my first CNN kernel and as such, I believe the [Digit Recognizer dataset/competition](https://www.kaggle.com/c/digit-recognizer) is a very suitable set of images for a beginner CNN project, considering the image size is homogeneous across all images (not common in real-world problems), that the size is small (28x28) so no resizing required, they are in grayscale and they are already in a csv, which can be easily read into a dataframe. \n\n<img src=\"http://img1.imagilive.com/0717/mnist-sample.png\" ></img>\n\nGiven the comfort that this dataset provides and taking inspration from very popular kernels such as [yassineghouzam's kernel](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6) and [poonaml's kernel](https://www.kaggle.com/poonaml/deep-neural-network-keras-way) among others, I've created my own kernel joining what I have found most useful from each kernel, as well as adding what I have learnt in the process and other notes that may be helpful for others or for future me.\n\nThe kernel consists in 3 main parts:\n    * Data preparation\n Firstly, even if the input data is already quite clean as mentioned before, it still needs some preparation and pre-processing in order to be in an appropriate format to then later be fed to the NN. This includes data separation, reshaping and visualization which might give insight to the data scientist as to the nature of the images.\n    * CNN\nAfterwards, the NN is defined (this is where Keras comes in), the convolutional steps added, NN parameters initialized, and the model trained. This part takes the most time in a ML project.\n    * Evaluation\nOnce the model is trained, it's interesting to evaluate the model performance by seeing the progress of the loss and extract some conclusions, that the model is overfitting, or if there is high variance for instance.\n\nIf you find some errors in theoretical concepts, comments of any kind or suggestions, please do let me know :)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# import libraries\n\nimport numpy as np # linear algebra, matrix multiplications\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# 1. Data pre-processing\n## 1.1 Load data\n\n    * train\nthis is the data used to train the CNN.  \nthe image data and their corresponding class is provided.   \nthe CNN learns the weights to create the mapping from the image data to their corresponding class.  \n\n    * test\nthis is the data used to test the CNN.  \nonly the image data is provided.  \nthe prediction is submitted to the competition and depending on the accuracy, a score is obtained."},{"metadata":{"trusted":true,"_uuid":"5c4c2fa7eeffa51a4c2294cac109bed54fda7dca","collapsed":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6dca7aa2c39302f7f2e568c01e7d44ecbf12dc1a"},"cell_type":"markdown","source":"## 1.2. Check shape, data type\n\n    * train\nthe train dataframe contains data from 42k images.  \nthe data from each image is streched out in 1D with 28*28 = 784 pixels.  \nthe first column is the label/class it belongs to, the digit it represents.  \n\n    * test\nthe test dataframe contains data from 28k images.  \nthis data shall be fed to the CNN so that it's new data, that the CNN has never seen before.  \nsame as in the train dataset, image data is streched out in 1D with 784 pixels.  \nthere is no label information, that is the goal of the competition, predicting labels as well as possible."},{"metadata":{"trusted":true,"_uuid":"8a7cb2975ffe4514b061c379a9bf8cc37c48f9ed","collapsed":true},"cell_type":"code","source":"# check data\n\nprint(train.shape)\nntrain = train.shape[0]\n\nprint(test.shape)\nntest = test.shape[0]\n\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ffd9d5ea1aa332b428fa668215c7c84c28f3dae","collapsed":true},"cell_type":"code","source":"# check data type\nprint(train.dtypes) # all int64, otherwise do train = train.astype('int64')\n\nprint(train.dtypes) # all int64, otherwise do test = test.astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c71f725e3a5fb0bcca79a6797f48556888797e7"},"cell_type":"markdown","source":"## 1.2 Extract xtrain, ytrain \nthe CNN will be fed xtrain and it will learn the weights to map xtrain to ytrain"},{"metadata":{"trusted":true,"_uuid":"f9d2f465a2abfab92687cd5c08404f7e8aef8ae6","collapsed":true},"cell_type":"code","source":"# extract xtrain, ytrain\n\n# array containing labels of each image\nytrain = train[\"label\"]\nprint(\"Shape of ytrain: \", ytrain.shape)\n\n# dataframe containing all pixels (the label column is dropped)\nxtrain = train.drop(\"label\", axis=1)\n\n# the images are in square form, so dim*dim = 784\nfrom math import sqrt\ndim = int(sqrt(xtrain.shape[1]))\nprint(\"The images are {}x{} squares.\".format(dim, dim))\n\nprint(\"Shape of xtrain: \", xtrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e5f24bd732ba81028c32812eccec43ce50db629","collapsed":true},"cell_type":"code","source":"ytrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4b74cdf8150c9a6a4e6910bfa0f1441162d4f19"},"cell_type":"markdown","source":"## 1.3. Mean and std of the classes"},{"metadata":{"trusted":true,"_uuid":"6eb1fc174dc86ce277e2b8fc858ea1067c8174ee","collapsed":true},"cell_type":"code","source":"import seaborn as sns\nsns.set(style='white', context='notebook', palette='deep')\n\n# plot how many images there are in each class\nsns.countplot(ytrain)\n\nprint(ytrain.shape)\nprint(type(ytrain))\n\n# array with each class and its number of images\nvals_class = ytrain.value_counts()\nprint(vals_class)\n\n# mean and std\ncls_mean = np.mean(vals_class)\ncls_std = np.std(vals_class,ddof=1)\n\nprint(\"The mean amount of elements per class is\", cls_mean)\nprint(\"The standard deviation in the element per class distribution is\", cls_std)\n\n# 68% - 95% - 99% rule, the 68% of the data should be cls_std away from the mean and so on\n# https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule\nif cls_std > cls_mean * (0.6827 / 2):\n    print(\"The standard deviation is high\")\n    \n# if the data is skewed then we won't be able to use accurace as its results will be misleading and we may use F-beta score instead.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33027153afbdb5887955c8f39e29c000e374b573"},"cell_type":"markdown","source":"### Summary:\n\nShape of xtrain is: (42000, 784)  \nShape of ytrain is: (42000, )  \nShape of test is: (28000, 784)  \n\nnumber of classes = 10, the distribution of the pictures per class has a mean of 4200 images and a std of 237 images.     \nThe digit 1 has the most representation (4684 images) and the digit 5 the least (3795 images). This data can be seen by printing *vals_class*  \nThis corresponds to a small percentage (5.64%) so there is no class imbalance.  "},{"metadata":{"_uuid":"7b07fc7612d5ce54f82d9debdce7273fd9a3f735"},"cell_type":"markdown","source":"## 1.4. Check nuls and missing values"},{"metadata":{"trusted":true,"_uuid":"03c7f67d1575a25008ebd244872a262f4a4495ce","collapsed":true},"cell_type":"code","source":"# Check the data\n# df.isnull() returns a boolean df with true if value is NaN and false otherwise\n# df.isnull().any() returns a df with 1 col and ncol rows where each row says if there is a NaN value present in that col\n# df.isnull().any().any() returns a bool with True if any of the df.isnull().any() rows is True\n\ndef check_nan(df):\n    print(df.isnull().any().describe())\n    print(\"There are missing values\" if df.isnull().any().any() else \"There are no missing values\")\n\n    if df.isnull().any().any():\n        print(df.isnull().sum(axis=0))\n        \n    print()\n        \ncheck_nan(xtrain)\ncheck_nan(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c960d91af825d5cf5d291ef019b779876e5ccb06"},"cell_type":"markdown","source":"## 1.5. Visualization"},{"metadata":{"trusted":true,"_uuid":"9a22f34f81cb94af5787f06448e326058e61122d","collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\n\n# convert train dataset to (num_images, img_rows, img_cols) format in order to plot it\nxtrain_vis = xtrain.values.reshape(ntrain, dim, dim)\n\n# https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html\n# subplot(2,3,3) = subplot(233)\n# a grid of 3x3 is created, then plots are inserted in some of these slots\nfor i in range(0,9): # how many imgs will show from the 3x3 grid\n    plt.subplot(330 + (i+1)) # open next subplot\n    plt.imshow(xtrain_vis[i], cmap=plt.get_cmap('gray'))\n    plt.title(ytrain[i]);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2051502a7a5443f14679c836f5f3bfabd8d0bdac"},"cell_type":"markdown","source":"## 1.6. Normalization\nPixels are represented in the range [0-255], but the NN converges faster with smaller values, in the range [0-1].\n"},{"metadata":{"trusted":true,"_uuid":"76e6f0a067bb7cabf2cfa079a3917f2391693872","collapsed":true},"cell_type":"code","source":"# Normalize the data\nxtrain = xtrain / 255.0\ntest = test / 255.0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e07e4cdd4acf0ae14bb0dcb633fd2e020ff3e3d6"},"cell_type":"markdown","source":"## 1.7. Reshape"},{"metadata":{"trusted":true,"_uuid":"9685eb7e78736876af2f032c53dd2f2d2ea5382b","collapsed":true},"cell_type":"code","source":"# reshape of image data to (nimg, img_rows, img_cols, 1)\ndef df_reshape(df):\n    print(\"Previous shape, pixels are in 1D vector:\", df.shape)\n    df = df.values.reshape(-1, dim, dim, 1) \n    # -1 means the dimension doesn't change, so 42000 in the case of xtrain and 28000 in the case of test\n    print(\"After reshape, pixels are a 28x28x1 3D matrix:\", df.shape)\n    return df\n\nxtrain = df_reshape(xtrain) # numpy.ndarray type\ntest = df_reshape(test) # numpy.ndarray type","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d94d64556e53d62e3822542f4575d82f1132bd28"},"cell_type":"markdown","source":"### Note\n\nIn real world problems, the dimensions of images could diverge from this particular 28x28x3 set in two ways:\n   * Images are usually much bigger.\n\nIn this case all images are 28x28x1, but in another problem I'm working on, I have images of 3120x4160x3, so much bigger and in RGB. Usually images are resized to much smaller dimensions, in my case I'm resizing them to 64x64x3 but they can be made much smaller depending on the problem. In this MNIST dataset there is no such problem since the dimensions are already small.\n   * Images don't usually have the same dimensions\n   \nDifferent dimension images are a problem since dense layers at the end of the CNN have a fixed number of neurons, which cannot be dynamically changed. This means that the layer expects fixed image dimensions, which means all images must be resized to the same dimensions before training. There is another option, namely, using a FCN (fully convoluted network) which consits solely of convolutional layers and a very big pooling in the end, so each image can be of any size, but this architecture isn't as popular as the CNN + FC (fully connected) layers which is the one I'm familiarized with.  \nThere are various methods to make images have the same dimensions:\n   * resize to a fixed dimension\n   * add padding to some images and resize\n   * ...  \n\nIn my other problem I have scanned pictures, so I trim the whitespace and resize afterwards. Being this a friendly dataset, all digits are the same size and wel centered so no need to worry about resizing.\n\n"},{"metadata":{"_uuid":"ac10231c13c9f52440bbb76ed6bc285c402d9b5e"},"cell_type":"markdown","source":"## 1.8. One hot encoding of label\n\nCurrently the labels vary in the range [0-9] which is intuitive, but in order to define the type of loss for the NN later, which in this case is categorical_crossentropy (reason is explained in 3.3,\nTODO: expand this, correct subsection\nthe targets should be in categorical format (=one hot-vectors): ex : 2 -> [0,0,1,0,0,0,0,0,0,0]\n\nytrain before  \n0    1  \n1    0  \n2    1  \n3    4  \n4    0  \n\nytrain after  \n[[0. 1. 0. ... 0. 0. 0.]  \n [1. 0. 0. ... 0. 0. 0.]  \n [0. 1. 0. ... 0. 0. 0.]  \n ...  \n [0. 0. 0. ... 1. 0. 0.]  \n [0. 0. 0. ... 0. 0. 0.]  \n [0. 0. 0. ... 0. 0. 1.]]\n"},{"metadata":{"trusted":true,"_uuid":"508a23dfec7dde6ea68e2904277687283a956327","collapsed":true},"cell_type":"code","source":"from keras.utils.np_utils import to_categorical\n\nprint(type(ytrain))\n# number of classes, in this case 10\nnclasses = ytrain.max() - ytrain.min() + 1\n\nprint(\"Shape of ytrain before: \", ytrain.shape) # (42000,)\n\nytrain = to_categorical(ytrain, num_classes = nclasses)\n\nprint(\"Shape of ytrain after: \", ytrain.shape) # (42000, 10), also numpy.ndarray type\nprint(type(ytrain))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e078f5545f1387ae0b49f14fee7ac1d47e5bcae"},"cell_type":"markdown","source":"## 1.9. Split training and validation sets\n\nThe available data is 42k images.  If the NN is trained with these 42k images, it might overfit and respond poorly to new data.  \nIn order to check this before actually submitting the predictions and risking a bad performance, a small percentage of the train data is separated and named validation data.  \nThe ratio of the split can vary from 10% in small datasets to 1% in cases with 1M images.\n\nThe NN is then trained with the remaining of the training data, and in each step (actually epoch), keras shows how the NN does with this new data. This is the important metric.  \nThe NN might do very well with trained data but the goal is that the NN has a good performance with new data. If the NN does well with val data, it's probable that it will do well with the test data. (more in this in section 3.3)\n\nrandom_state in train_test_split ensures that the data is pseudo-randomly divided.  \nIf the images were ordered by class, activating this feature guarantees their pseudo-random split.  \nThe seed means that every time this pseudo-randomization is applied, the distribution is the same.\n\nstratify in train_test_split ensures that there is no overrepresentation of classes in the val set.  \nIt is used to avoid some labels being overrepresented in the val set.   \nNote: only works with sklearn version > 0.17"},{"metadata":{"trusted":true,"_uuid":"117f9749a1511c0a238a0dece0f684bd27b41cef","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# fix random seed for reproducibility\nseed = 2\nnp.random.seed(seed)\n\n# percentage of xtrain which will be xval\nsplit_pct = 0.1\n\n# Split the train and the validation set\nxtrain, xval, ytrain, yval = train_test_split(xtrain,\n                                              ytrain, \n                                              test_size=split_pct,\n                                              random_state=seed,\n                                              stratify=ytrain\n                                             )\n\nprint(xtrain.shape)\nprint(ytrain.shape)\nprint(xval.shape)\nprint(yval.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"237545703337e0e18dc1a7f40daa9edff5d9dec3"},"cell_type":"markdown","source":"### Summary\n\nThe available data is now divided as follows:\n* **Train data**: images (xtrain) and labels (ytrain), 90% of the available data\n* **Validation data**: images (xval) and labels (yval), 10% of the available data"},{"metadata":{"_uuid":"a6f751f76f859c7b51b846f81dc79baa7609e934"},"cell_type":"markdown","source":"# 2. CNN"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"162a9388b958cf25272870d290a616be47fd85ee"},"cell_type":"code","source":"from keras import backend as K\n\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Lambda, Flatten, BatchNormalization\nfrom keras.layers import Conv2D, MaxPool2D, AvgPool2D","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ee343a11f8881814ba57a48bdf1a69752c7cc34"},"cell_type":"markdown","source":"## 2.1. Define the model\n\nSet the CNN model \nmy CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n\nvalid padding: no padding, the img shrinks: n - f + 1\nsame padding: padding of 2, the img doesn't shrink: p = (f-1)/2 --> (n+2) - f(=3) + 1 = n\nstrided conv jumps with strides: n_ = (n + 2p - f)/s + 1\n\npooling layer, max/avg pooling: 2 hyperparameters (f: size of filter, s:stide) but no parameters to learn\nwhy? to reduce variance, reduce computation complexity.\nMax pooling extracts the most important features like edges whereas, average pooling extracts features so smoothly. (ane): max pooling better for grayscale then\n\ndropout good for regularization. the net becomes less sensitive to the specific weights of neurons. capable of better generalization and less likely to overfit the train data.\noptimal value for dropout=0.2 https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5\nlocal minima in acc and loss. #iterations required to converge is higher, training time for each epoch is less\nthe convolutional layers usually don't have all that many parameters, so they need less regularization to begin with --> litle dropout. 0.5 in FC layers\nbatch normalization"},{"metadata":{"trusted":true,"_uuid":"bb1b773c6bf7baf33756dd265f3a2c1bc662b220","collapsed":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(dim,dim,1))) # The relu activation function is represented mathematically by max(0,X\n#model.add(BatchNormalization())\nmodel.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu',))\n#model.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters=64, kernel_size=(5,5), padding='same', activation='relu'))\n#model.add(BatchNormalization())\nmodel.add(Conv2D(filters=64, kernel_size=(5,5), padding='same', activation='relu'))\n#model.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\nmodel.add(Dense(120, activation='relu'))\n#model.add(Dropout(0.5))\nmodel.add(Dense(84, activation='relu'))\n#model.add(Dropout(0.5))\nmodel.add(Dense(nclasses, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7983fec4500a27ff54adc40535405ba71e9fd80a","collapsed":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5785f24f35271268d9616e45cdde37ddf4547921"},"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cb57a110e90a1006995534819c581c16a7b645c"},"cell_type":"markdown","source":"We have used categorical_crossentropy as the cost function for that model but what does we mean by cost function\n\nCost function : It is a measure of the overall loss in our network after assigning values to the parameters during the forward phase so it indicates how well the parameters were chosen during the forward probagation phase.\nOptimizer : It is the gradiant descent algorithm that is used. We use it to minimize the cost function to approach the minimum point. We are using adam optimizer which is one of the best gradient descent algorithms. You can refere to this paper to know how it works https://arxiv.org/abs/1412.6980v8\nYou can use other metrics to measure the performance other than accuracy as precision or recall or F1 score. the choice depends on the problem itself. Where high recall means low number of false negatives , High precision means low number of false positives and F1 score is a trade off between them. You can refere to this article for more about precision and recall http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n\n(from coursera audio) However, accuracy isn't a great metric for this task, since the labels are heavily skewed to 0's, so a neural network that just outputs 0's would get slightly over 90% accuracy. We could define more useful metrics such as F1 score or Precision/Recall. But let's not bother with that here, and instead just empirically see how the model does."},{"metadata":{"trusted":true,"_uuid":"e8a38a240404358164f63020aceb591c3907ffdc","collapsed":true},"cell_type":"code","source":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2fd7356e907ad7001a84890191271f59975ff49e"},"cell_type":"code","source":"epochs = 2\nbatch_size = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec0fa1f7edbfd1c834725c4225c485a1cb13b1c8","collapsed":true},"cell_type":"code","source":"xtrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1da40a349de35f1c2d4ce96c61c81d0091efc23","collapsed":true},"cell_type":"code","source":"ytrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1379cb5bd05278c3726b39df441a5eb1dcd9dc5","collapsed":true},"cell_type":"code","source":"# With data augmentation to prevent overfitting\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(xtrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5b3c58b2564bdc98ef8f7f949c164ec95b5e876","collapsed":true},"cell_type":"code","source":"xtrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a87fb83b3d6654ee0b2a54e65a8b9c00ec48cf4","scrolled":false,"collapsed":true},"cell_type":"code","source":"# Fit the model\nhistory = model.fit_generator(datagen.flow(xtrain,ytrain, batch_size=batch_size),\n                              epochs=epochs, validation_data=(xval,yval),\n                              verbose=1, steps_per_epoch=xtrain.shape[0]//batch_size, \n                              callbacks=[learning_rate_reduction])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5876a7e97bfb916020e9c04a6a6056bc96f98669","collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\n\n# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)\n\n# overfitted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63a0bda9010ac311a6a9c845343c64ebcba8ddc3","collapsed":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport itertools\n\n# Confusion matrix\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nypred = model.predict(xval)\n# Convert predictions classes from one hot vectors to labels: [0 0 1 0 0 ...] --> 2\nypred_classes = np.argmax(ypred,axis=1)\n# Convert validation observations from one hot vectors to labels\nytrue = np.argmax(yval,axis=1)\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(ytrue, ypred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes=range(nclasses))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"242868c9a91318dd86b2222483eaddc09c7b08e8"},"cell_type":"code","source":"# conf matrix opt2\nX_train, X_validate, y_train, y_validate = train_test_split(features, labels, test_size=0.1)\npredicted = np.argmax(model.predict_generator(test_generator(X_validate), steps=X_validate.shape[0]), axis=1)\ntmp = pd.DataFrame(sklearn.metrics.confusion_matrix(y_validate, predicted))\nplt.subplots(figsize=(10,10)) \nsns.heatmap(tmp, annot=True, fmt='.1f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5f8df7abc5d6278caa4098f5d39ea7566a55ccb","collapsed":true},"cell_type":"code","source":"# Display some error results \n\n# Errors are difference between predicted labels and true labels\nerrors = (ypred_classes - ytrue != 0) # array of bools where the equality is true or false \n\nypred_er = ypred[errors]\nypred_classes_er = ypred_classes[errors]\nytrue_er = ytrue[errors]\nxval_er = xval[errors]\n\ndef display_errors(errors_index, img_errors, pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows, ncols, sharex=True, sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n            \n# Probabilities of the wrong predicted numbers\nypred_er_prob = np.max(ypred_er,axis=1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_er = np.diagonal(np.take(ypred_er, ytrue_er, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_er = ypred_er_prob - true_prob_er\n\n# Sorted list of the delta prob errors\nsorted_delta_er = np.argsort(delta_pred_true_er)\n\n# Top 6 errors \nmost_important_er = sorted_delta_er[-12:-6]\n\n# Show the top 6 errors\ndisplay_errors(most_important_er, xval_er, ypred_classes_er, ytrue_er)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f85a38f96f2fb9c26119f6a2a32ce84ee0751c91","collapsed":true},"cell_type":"code","source":"predictions = model.predict_classes(test, verbose=1)\n\nsubmissions = pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                         \"Label\": predictions})\nsubmissions.to_csv(\"mnist0208.csv\", index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"36e50badf8a5683d1e567de0bca4fbe5ab14441c"},"cell_type":"code","source":"# https://www.kaggle.com/cdeotte/how-to-choose-cnn-architecture-mnist\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
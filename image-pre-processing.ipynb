{"cells":[{"metadata":{"_uuid":"1e0b4f8b8f73685275a78b4923f996b437e6cc98"},"cell_type":"markdown","source":"# Image processing: PIL, imagenio, openCV\n<hr>\nLast version date: 12-09-2018\n<hr>\n\n## Table of Contents\n\n1. **Initial exploration**  \n2. **Open images with PIL Image**  \n   2.1. Blur  \n   2.2. Grayscale / RGB  \n   2.3. Resize / crop  \n   2.4. Enhance  \n   2.5. Binarize labels  \n3. **Open images with imageio**  \n4. **Open images with openCV**  \n   4.1. Trim borders  \n   4.2. Resize  \n   4.3. Calculate histogram  \n   4.4. Equalize histogram  \n   4.5. 2D histogram  \n   4.6. CLAHE  \n   4.7. Adaptive Gaussian thresholding  \n   4.8. Erosion  \n   4.9. Opening  \n   4.10. Closing  \n   4.11. Calculate image difference"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42b665f9808a484940d28d5abe2777a93d4e12f3"},"cell_type":"markdown","source":"## 1. Initial exploration"},{"metadata":{"trusted":true,"_uuid":"34238283ce7f2365f14c0e4656ed113431134eea","collapsed":true},"cell_type":"code","source":"!ls ../input/flowers/flowers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e40a2f04797032445267fcbee79b278ccb0e9ad","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"train_dir = \"../input/flowers/flowers/\"\nclasses = os.listdir(train_dir)\nclasses = sorted(classes)\nnclasses = len(classes)\nclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37a08c2ad016c93c9646ef5d17fcce0cc15ccb62","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"for _class in classes:\n    print('{} {} images'.format(_class, len(os.listdir(os.path.join(train_dir, _class)))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdf759bbbaaa566e5411e7aa056fb11e9096fadd"},"cell_type":"markdown","source":"## 2. Open images with PIL Image"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c9b89ca8312f11625e10813cbd30376eb052cd36"},"cell_type":"code","source":"from PIL import Image, ImageOps, ImageFilter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e841909a268bf6c1e7b5ad3db0e50de18d839217"},"cell_type":"code","source":"other_extensions = [\".db\", \".pyc\", \".py\"]\ndef open_images_pil(path, classes, dim=32):\n    \n    xall = []\n    yall = []\n    label = 0\n    j = 0\n\n    for cl in classes:\n        clsdir = os.path.join(path, cl)\n        for imgname in os.listdir(clsdir):\n            bad_ext_found = 0\n            for other_ext in other_extensions:\n                if imgname.endswith(other_ext):\n                    bad_ext_found = 1\n                    break\n            if not bad_ext_found:\n                print(\"Opening files in {}: {}\".format(cl, str(j + 1)), end=\"\\r\")\n                imgpath = os.path.join(clsdir, imgname)\n\n                #open and pre-process images\n                img = Image.open(imgpath)\n                img = ImageOps.fit(img, (dim, dim), Image.ANTIALIAS).convert('RGB')\n                \n                xall.append(img)  # Get image \n                yall.append(label)  # Get image label (folder name)\n                j += 1\n\n        j = 0\n        label += 1\n        print()\n\n    n = len(xall)\n    print(\"{} images in set\".format(n))\n    return xall, yall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"24c6c9bbb12b049d08eaab166cf37a73a467c4ee","collapsed":true},"cell_type":"code","source":"xall, yall = open_images_pil(train_dir, classes, 256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8704a82dd33c9d0c7e06e4031be44945c08aabe9","collapsed":true},"cell_type":"code","source":"im = xall[0]\nim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"829c61a4cc364e0762a2bac81a1a1b6eca602293","collapsed":true},"cell_type":"code","source":"im.format, im.size, im.mode","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c5f4c2be0757f99a32076aa49b104e489099ee4"},"cell_type":"markdown","source":"### 2.1. Blur"},{"metadata":{"trusted":true,"_uuid":"176c2f1ba5fc08b685948dfd6a786cfc740c5363","collapsed":true},"cell_type":"code","source":"blur = im.filter(ImageFilter.BLUR)\nblur","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ed96ae2544385c3b47ca4383f4cec762d73497f"},"cell_type":"markdown","source":"### 2.2. Grayscale and RGB"},{"metadata":{"trusted":true,"_uuid":"5cbfbbfb4c598465a72c7e753b7e6c8d8af81cb4","collapsed":true},"cell_type":"code","source":"gray = im.convert('L')\nrgb = gray.convert(\"RGB\")\ngray","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cacc6a72b217d6c5459934cf683c572d8244cc54"},"cell_type":"markdown","source":"### 2.3. Resize / crop"},{"metadata":{"trusted":true,"_uuid":"f7c7a579a36acdc72663c62f2648cccb8898024f","collapsed":true},"cell_type":"code","source":"box = (100, 100, 400, 400)\ncrpd = im.crop(box)\ncrpd","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12029142856f2a25c49456f5e60c4af4e3dc0d53"},"cell_type":"markdown","source":"### 2.4. Enhance"},{"metadata":{"trusted":true,"_uuid":"0300891d8fd93188c7e3b8dbefaac0c93b08ddbc","collapsed":true},"cell_type":"code","source":"enh = im.filter(ImageFilter.DETAIL)\nenh","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a1ec1e6a5123b2c323a1ae67474b9041e7dbfb9a"},"cell_type":"code","source":"_xall = np.stack(xall, axis=0) # from list of len 56 of ndarrays (dim, dim, 3) to ndarray (52, dim, dim, 3)\n_xall.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e37884da7bc9c0fa80f5f35462fec1bac48029e3"},"cell_type":"markdown","source":"### 2.5. Binarize label"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cb5b41227f1319b9192b4b2a851cfa74f78b3a48"},"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\nlb = LabelBinarizer().fit(yall)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c024720cc6628dd8690d8e086ffc33c2777b7538","collapsed":true},"cell_type":"code","source":"label = lb.transform(ori_label) \nlabel.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78417ec1ca171d822ee9595a4b96db48bf400688","collapsed":true},"cell_type":"code","source":"_xall = np.asarray(xall)\n\nfor i in range(0,9): # how many imgs will show from the 3x3 grid\n    plt.subplot(330 + (i+1)) # open next subplot\n    plt.imshow(_xall[i + 155], cmap=plt.get_cmap('gray'))\n    plt.title(yall[i + 155]);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"addb4b678b55711b9bfb853fe830f36a5afb09c5"},"cell_type":"markdown","source":"## 3. Open images with imageio"},{"metadata":{"trusted":true,"_uuid":"950788dc5228b3c7d82d21f40a75f56d6c48e592","collapsed":true},"cell_type":"code","source":"import imageio\nfrom skimage.transform import resize\n\nclasspath = os.path.join(train_dir, classes[0])\nimgpath = os.listdir(classpath)[0] # first imagepath of first class, as example\nimgpath = os.path.join(classpath, imgpath)\n\nimg = imageio.imread(imgpath)\nimg = resize(img, (dim, dim, 3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a34bfb60c26927628f42f172c078edeb09d9d4cd"},"cell_type":"markdown","source":"## 4. Open images with OpenCV (cv2)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"789b2db11a495658a4023eebef5e0b6af8d64bd9"},"cell_type":"code","source":"import cv2\nfrom cv2 import imread, cvtColor, resize, threshold, calcHist, equalizeHist","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fda3bbf51916f5da9ac30fda34c6fa629a049a8"},"cell_type":"markdown","source":"### 4.1. Trim margin"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9f60a830af29b759bfc2b6840aeee60743a969e9"},"cell_type":"code","source":"def trim_margin(img, lim):\n    return img[lim:-lim, lim:-lim]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24711437d64b8c2f1c5d3d6fb815f5406b44db45"},"cell_type":"markdown","source":"### 4.2. Resize"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6890a41db5227f9866ba4474be54671f2187f40c"},"cell_type":"code","source":"supported_dims = [16, 32, 64, 128, 256]\ndef img_resize(img, dims):\n    if dims in supported_dims:\n        return cv2.resize(img, (dims, dims))\n    else:\n        print(\"Incorrect image dimensions.\\n\")\n        return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6f1a58c7c3402dc81305747e53d55db89dd30ea9"},"cell_type":"code","source":"other_extensions = [\".db\", \".pyc\", \".py\"]\ndef open_images_cv2(path, classes, dim=32):\n    \n    xall = []\n    yall = []\n    label = 0\n    j = 0\n\n    for cl in classes:\n        clsdir = os.path.join(path, cl)\n        for imgname in os.listdir(clsdir):\n            bad_ext_found = 0\n            for other_ext in other_extensions:\n                if imgname.endswith(other_ext):\n                    bad_ext_found = 1\n                    break\n            if not bad_ext_found:\n                print(\"Opening files in {}: {}\".format(cl, str(j + 1)), end=\"\\r\")\n                imgpath = os.path.join(clsdir, imgname)\n\n                #open and pre-process images\n                img = imread(imgpath, cv2.IMREAD_COLOR)\n                img = cvtColor(img, cv2.COLOR_BGR2RGB)\n                img = trim_margin(img, int(img.shape[0] * 0.05))\n                img = img_resize(img, dim)\n                #img = equalize_hist(img)\n\n                xall.append(img)  # Get image \n                yall.append(label)  # Get image label (folder name)\n                j += 1\n\n        j = 0\n        label += 1\n        print()\n\n    n = len(xall)\n    print(\"{} images in set\".format(n))\n    return xall, yall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9679b31207596474bd835efd94b24aa54b687421","collapsed":true},"cell_type":"code","source":"xall, yall = open_images_cv2(train_dir, classes, 256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a1c89062408fa39100a9af0cba29a8dd072d47d4"},"cell_type":"code","source":"xall = np.asarray(xall)\nyall = np.asarray(yall)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eaf83154ac09ce3efffec2bd575a62382bfae2a0","collapsed":true},"cell_type":"code","source":"image = xall[0]\nplt.imshow(image, cmap=plt.get_cmap('gray'))\nplt.title(yall[0]);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4eaad19f4cd8f6200aa63734682d295a14de2a06"},"cell_type":"markdown","source":"### 4.3. Calculate histogram\n```python\ncv2.calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate]])\n```\n\n* **images** : it is the source image of type uint8 or float32. it should be given in square brackets, ie, “[img]”.\n* **channels** : it is also given in square brackets. It the index of channel for which we calculate histogram. For example, if input is grayscale image, its value is [0]. For color image, you can pass [0],[1] or [2] to calculate histogram of blue,green or red channel respectively.\n* **mask** : mask image. To find histogram of full image, it is given as “None”. But if you want to find histogram of particular region of image, you have to create a mask image for that and give it as mask. (I will show an example later.)\n* **histSize** : this represents our BIN count. Need to be given in square brackets. For full scale, we pass [256].\n* **ranges** : this is our RANGE. Normally, it is [0,256].\n* **[Other references][1]**\n\n[1]: https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_histograms/py_histogram_begins/py_histogram_begins.html"},{"metadata":{"trusted":true,"_uuid":"92d608ce41b37b9141e1a78fa8b84966ea6a8c3d","collapsed":true},"cell_type":"code","source":"color = ('b','g','r')\nfor i,col in enumerate(color):\n    histr = cv2.calcHist([image],[i],None,[256],[0,256])\n    plt.plot(histr,color = col)\n    plt.xlim([0,256])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a96d797c381d2ac68e407268ae35a6c902cc7c0a"},"cell_type":"markdown","source":"### 4.4. Equalize histogram\n\n* **channels** = [0,1] because we need to process both H and S plane.\n* **bins** = [180,256] 180 for H plane and 256 for S plane.\n* **range** = [0,180,0,256] Hue value lies between 0 and 180 & Saturation lies between 0 and 256.\n* **[Other references][1]**\n\n[1]: https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_histograms/py_2d_histogram/py_2d_histogram.html"},{"metadata":{"trusted":true,"_uuid":"419e324c8ab03f549aff4394363d783178855ad0","collapsed":true},"cell_type":"code","source":"img_yuv = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n\n# equalize the histogram of the Y channel\nimg_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n\n# convert the YUV image back to RGB format\nimgo = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2RGB)\n\nplt.imshow(imgo)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa4896e94b49bab8ef1b250b66e4af46200d8e12","collapsed":true},"cell_type":"code","source":"color = ('r','g','b')\nfor i,col in enumerate(color):\n    histr = cv2.calcHist([imgo],[i],None,[256],[0,256])\n    print(max(histr))\n    plt.plot(histr,color=col)\n    plt.xlim([0,256])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df6eb0ccad34355cc1f0e39793ef9c7c3b3bf9ce"},"cell_type":"markdown","source":"### 4.5. 2D histogram"},{"metadata":{"trusted":true,"_uuid":"df9565e262d2ffb150cc365b0be97745acb7a8cb","collapsed":true},"cell_type":"code","source":"# https://lmcaraig.com/image-histograms-histograms-equalization-and-histograms-comparison/\n\nfrom matplotlib import ticker\n\nbins = 256\ntick_spacing = 5\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 5))\nchannels_mapping = {0: 'B', 1: 'G', 2: 'R'}\nfor i, channels in enumerate([[0, 1], [0, 2], [1, 2]]):\n    hist = cv2.calcHist([image], channels, None, [bins]*2, [0, 256]*2)\n\n    channel_x = channels_mapping[channels[0]]\n    channel_y = channels_mapping[channels[1]]\n\n    ax = axes[i]\n    ax.set_xlim([0, bins - 1])\n    ax.set_ylim([0, bins - 1])\n\n    ax.set_xlabel(f'Channel {channel_x}')\n    ax.set_ylabel(f'Channel {channel_y}')\n    ax.set_title(f'2D Color Histogram for {channel_x} and {channel_y}')\n\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n\n    im = ax.imshow(hist)\n\nfig.colorbar(im, ax=axes.ravel().tolist(), orientation='orizontal')\nfig.suptitle(f'2D Color Histograms with {bins} bins', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26b76f092a92be7c6b4ad73874b6c5d9fa7cd02b"},"cell_type":"markdown","source":"### 4.6 Contrast Limited Adaptive Histogram Equalization (CLAHE)\n\n[From Github][1]\n\n[1]: https://github.com/jagracar/OpenCV-python-tests/blob/master/OpenCV-tutorials/imageProcessing/histogramEqualization2.py\n"},{"metadata":{"trusted":true,"_uuid":"cf4fe5c299025dbb5315eafc8533954e7deb0eed","collapsed":true},"cell_type":"code","source":"clahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(8, 8))\ngray = cvtColor(image, cv2.COLOR_RGB2GRAY)\nclaheImg = clahe.apply(gray)\nplt.imshow(claheImg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9e406a66e71d6cd82301c1b3acfcde94cfd7d62c"},"cell_type":"markdown","source":" ### 4.7. Adaptive Gaussian thresholding\n \n [From opencv tutorial][1]\n \n[1]: https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html"},{"metadata":{"trusted":true,"_uuid":"f741d1634dc540ac6dd847ed372957d4e843ee46","collapsed":true},"cell_type":"code","source":"imgpath = \"../input/files/files/unsplash/-537308-unsplash.jpg\"\n\ngray\nimg = cv2.medianBlur(gray,5)\n\nret, th1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY)\nth2 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY,11,2)\nth3 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,11,2)\n\ntitles = ['Original Image', 'Global Thresholding (v = 127)',\n            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']\nimages = [img, th1, th2, th3]\n\nfor i in range(4):\n    plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')\n    plt.title(titles[i])\n    plt.xticks([]),plt.yticks([])\nplt.show()\n\n# doesn't look very good","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d891bde81e23496eb1a08c9409e5e569e562048"},"cell_type":"markdown","source":"### 4.8. [Erosion][1]\n\n[1]: https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html"},{"metadata":{"trusted":true,"_uuid":"f3e96b4a7156a460907329d850baccc54ee89adb","collapsed":true},"cell_type":"code","source":"kernel = np.ones((5,5),np.uint8)\nerosion = cv2.erode(image, kernel, iterations=1)\n\nplt.subplot(1,2,1)\nplt.imshow(image)\nplt.subplot(1,2,2)\nplt.imshow(erosion)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf3120c327c2c88d4262c685a017430058f867ba"},"cell_type":"markdown","source":"### 4.9. Opening\n(same link)"},{"metadata":{"trusted":true,"_uuid":"bf516fbac46d5ae0d9dddcda0a942bca46af0afa","collapsed":true},"cell_type":"code","source":"opening = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\nplt.subplot(1,2,1)\nplt.imshow(image)\nplt.subplot(1,2,2)\nplt.imshow(opening)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41cea8a5d51948fcbad6a93549db2696d6876fe5"},"cell_type":"markdown","source":"### 4.10. Closing\n(same link)"},{"metadata":{"trusted":true,"_uuid":"98063ba4feebe9356ca77ea8cfb76f0d9a79965f","collapsed":true},"cell_type":"code","source":"closing = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\nplt.subplot(1,2,1)\nplt.imshow(image)\nplt.subplot(1,2,2)\nplt.imshow(closing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"99fb5be0d88635ee62a539a04304135f1d2bf2e9"},"cell_type":"markdown","source":"### 4.11. [Calculate image difference with opencv][1]\n\nThis value gives insight about how similar two images are.\n\n[1]: https://www.pyimagesearch.com/2017/06/19/image-difference-with-opencv-and-python/"},{"metadata":{"trusted":true,"_uuid":"b265f73433c0cc159d24023ba133ff27b0e1b004","collapsed":true},"cell_type":"code","source":"from skimage.measure import compare_ssim\nfrom skimage.transform import resize\n\nimage2 = xall[1]\n\n(score, diff) = compare_ssim(image, image2, full=True, multichannel=True)\ndiff = (diff * 255).astype(\"uint8\")\nprint(\"SSIM: {}\".format(score))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}